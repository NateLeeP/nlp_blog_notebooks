{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1967e801",
   "metadata": {},
   "source": [
    "<h1> Answering Questions with DistilBERT Transformer model </h1>\n",
    "<p> ChapGPT is built on the GPT3 transformer model, but DistilBERT is a reasonable approximation of GPT3. The Hugging Face library has a plethora of tools for building a NLP solution. This walkthrough provides a high-level (emphasis on high-level) overview of the fundamentals of Hugging Face, with links to the official documentation. We'll import a DistillBERT pretrained model, then improve it with a wiki answers dataset. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154217f4",
   "metadata": {},
   "source": [
    "<h3> Instantiating a Model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96c585",
   "metadata": {},
   "source": [
    "<p> Hugging face provides access to models through a 'pipeline'. Pipelines are an interface for interacting with an underlying model - objects instantiated with the 'pipeline' class accept inputs as arugments and outputs iteratable results Get started with pipelines <a href=\"https://huggingface.co/docs/transformers/v4.26.1/en/quicktour\">here</a> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c3911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'searching for his role', score: 0.519, start: 15, end: 37\n"
     ]
    }
   ],
   "source": [
    "### Import from transformers\n",
    "from transformers import pipeline, \n",
    "\n",
    "### Instantiate a pipeline object. \n",
    "question_answerer = pipeline(task=\"question-answering\")\n",
    "\n",
    "### Save results. A 'question_answering' model requires a 'quesiton' and 'context' argument\n",
    "result = question_answerer(question=\"What is the meaning of life?\", context=\"A young person searching for his role\")\n",
    "\n",
    "### Output results \n",
    "print(\n",
    "f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f184f",
   "metadata": {},
   "source": [
    "<h3> Customizing a model </h3>\n",
    "<p> Passing a task argument to pipeline lets Hugging Face pick the pre-trained model, but the pipeline provides further customization options. We'll take advantage of them to make use of our DistilBert model. Hugging Face provides  classes for specific models - a list of available models can be found <a href=https://huggingface.co/docs/transformers/v4.26.1/en/model_doc/auto#transformers.AutoModel.from_pretrained> here</a> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9504f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import and instantiate the GPDistilBert model. 'from_pretrained' accepts the name of the model as an argument. \n",
    "### The name is found at the top of the models 'model card'. https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads\n",
    "from transformers import DistilBertForQuestionAnswering, AutoTokenizer\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8d4b4",
   "metadata": {},
   "source": [
    "<h5> Tokenizers </h5>\n",
    "A model cannot accept generic text as input - it must accept 'tokenized' inputs. A tokenizer object <a href=\"https://huggingface.co/docs/transformers/main_classes/tokenizer\"> prepares the input</a> by converting the text into into 'tokens' the transformer reads. Each model has an associated tokenizer that can be instantiated by passing the model name to the <a href=\"https://huggingface.co/docs/transformers/v4.26.1/en/model_doc/auto#transformers.AutoTokenizer\">'AutoTokenizer'</a> Hugging Face class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b4df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "### now, we create a new question answerer out of our model\n",
    "\n",
    "question_answerer_distil_bert = pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer my question, print results\n",
    "result_distil_bert = question_answerer_distil_bert(question=\"What is the meaning of life?\", context=\"A young person searching for his role\")\n",
    "print(\n",
    "f\"Answer: '{result_distil_bert['answer']}', score: {round(result_distil_bert['score'], 4)}, start: {result_distil_bert['start']}, end: {result_distil_bert['end']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
